{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle LLM Science Exam Perplexity Ranking Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is based on the discussion and notebooks from [Psi](https://www.kaggle.com/code/philippsinger) and [Takamichi Toda](https://www.kaggle.com/takamichitoda)\n",
    "\n",
    "- https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/424242\n",
    "- https://www.kaggle.com/code/philippsinger/h2ogpt-perplexity-ranking\n",
    "- https://www.kaggle.com/code/takamichitoda/llm-perplexity-ranking-ensemble/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is, rather than using or training an LLM to predict the correct answer given the question and multiple choice answers, we instead:\n",
    "- Input each answer, prepended with the question, to a pretrained LLM and perform inference\n",
    "- For each token, see what the model predicts as its logprob\n",
    "- Calculate the perplexity over these results -- effectively treating the questions and multiple choice answers as a dataset and use the perplexity as the measure of how well the model is modelling the correct distribution that should correspond to the correct answer\n",
    "- Sort the perplexity for each of the question-answer pairs to get the answer as that with the lowest perplexity and use that as the model's answer to the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes ipywidgets python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9e7329105c4eb6883674a524b441ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login, notebook_login\n",
    "login()\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# TODO fix up for colab or kaggle if needed. No need for local or docker with volume mapped for HF cache\n",
    "# cwd = Path(os.getcwd())\n",
    "# data_dir =  cwd / 'data'\n",
    "# model_dir = [cwd / 'models']\n",
    "\n",
    "# GCP version\n",
    "cwd = Path('/workspace')\n",
    "data_dir =  cwd / 'data'\n",
    "model_dir = [cwd / 'models']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "COMPETITION='kaggle-llm-science-exam'\n",
    "\n",
    "# fix as needed\n",
    "# data_dir = Path(os.getcwd()) / 'data'\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "file_path = data_dir / f'{COMPETITION}.zip'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    # download dataset\n",
    "    subprocess.run(['kaggle', 'competitions', 'download', '-p', data_dir, '-c', COMPETITION], check=True)\n",
    "    # subprocess.run(['unzip', 'kaggle-llm-science-exam.zip'], check=True)\n",
    "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\"mistralai/Mistral-7B-v0.1\",\n",
    "          \"01-ai/Yi-6B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>MOND is a theory that reduces the observed mis...</td>\n",
       "      <td>MOND is a theory that increases the discrepanc...</td>\n",
       "      <td>MOND is a theory that explains the missing bar...</td>\n",
       "      <td>MOND is a theory that reduces the discrepancy ...</td>\n",
       "      <td>MOND is a theory that eliminates the observed ...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Which of the following is an accurate definiti...</td>\n",
       "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
       "      <td>Dynamic scaling refers to the non-evolution of...</td>\n",
       "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
       "      <td>Dynamic scaling refers to the non-evolution of...</td>\n",
       "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>The triskeles symbol was reconstructed as a fe...</td>\n",
       "      <td>The triskeles symbol is a representation of th...</td>\n",
       "      <td>The triskeles symbol is a representation of a ...</td>\n",
       "      <td>The triskeles symbol represents three interloc...</td>\n",
       "      <td>The triskeles symbol is a representation of th...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>What is the significance of regularization in ...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             prompt  \\\n",
       "0   0  Which of the following statements accurately d...   \n",
       "1   1  Which of the following is an accurate definiti...   \n",
       "2   2  Which of the following statements accurately d...   \n",
       "3   3  What is the significance of regularization in ...   \n",
       "4   4  Which of the following statements accurately d...   \n",
       "\n",
       "                                                   A  \\\n",
       "0  MOND is a theory that reduces the observed mis...   \n",
       "1  Dynamic scaling refers to the evolution of sel...   \n",
       "2  The triskeles symbol was reconstructed as a fe...   \n",
       "3  Regularizing the mass-energy of an electron wi...   \n",
       "4  The angular spacing of features in the diffrac...   \n",
       "\n",
       "                                                   B  \\\n",
       "0  MOND is a theory that increases the discrepanc...   \n",
       "1  Dynamic scaling refers to the non-evolution of...   \n",
       "2  The triskeles symbol is a representation of th...   \n",
       "3  Regularizing the mass-energy of an electron wi...   \n",
       "4  The angular spacing of features in the diffrac...   \n",
       "\n",
       "                                                   C  \\\n",
       "0  MOND is a theory that explains the missing bar...   \n",
       "1  Dynamic scaling refers to the evolution of sel...   \n",
       "2  The triskeles symbol is a representation of a ...   \n",
       "3  Regularizing the mass-energy of an electron wi...   \n",
       "4  The angular spacing of features in the diffrac...   \n",
       "\n",
       "                                                   D  \\\n",
       "0  MOND is a theory that reduces the discrepancy ...   \n",
       "1  Dynamic scaling refers to the non-evolution of...   \n",
       "2  The triskeles symbol represents three interloc...   \n",
       "3  Regularizing the mass-energy of an electron wi...   \n",
       "4  The angular spacing of features in the diffrac...   \n",
       "\n",
       "                                                   E answer  \n",
       "0  MOND is a theory that eliminates the observed ...      D  \n",
       "1  Dynamic scaling refers to the evolution of sel...      A  \n",
       "2  The triskeles symbol is a representation of th...      A  \n",
       "3  Regularizing the mass-energy of an electron wi...      C  \n",
       "4  The angular spacing of features in the diffrac...      D  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(data_dir / 'train.csv')\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    test_df = pd.read_csv(data_dir / 'test.csv', index_col='id')\n",
    "    # add unused answer to avoid errors when we're using the train set as test set\n",
    "    test_df[\"answer\"] = \"A\"\n",
    "else:\n",
    "    test_df = train_df.copy()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/philippsinger/h2ogpt-perplexity-ranking/notebook\n",
    "\n",
    "class Perplexity(nn.Module):\n",
    "    def __init__(self, reduce: bool = True):\n",
    "        super().__init__()\n",
    "        self.reduce = reduce\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        perplexity = []\n",
    "        for i in range(labels.shape[0]):\n",
    "            perplexity.append(self.loss_fn(shift_logits[i], shift_labels[i]))\n",
    "        perplexity = torch.stack(perplexity, dim=0)\n",
    "        # perplexity = torch.exp(perplexity)\n",
    "        if self.reduce:\n",
    "            perplexity = perplexity.mean()\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/philippsinger/h2ogpt-perplexity-ranking/notebook\n",
    "def precision_at_k(r, k):\n",
    "    \"\"\"Precision at k\"\"\"\n",
    "    assert k <= len(r)\n",
    "    assert k != 0\n",
    "    return sum(int(x) for x in r[:k]) / k\n",
    "\n",
    "def MAP_at_3(predictions, true_items):\n",
    "    \"\"\"Score is mean average precision at 3\"\"\"\n",
    "    U = len(predictions)\n",
    "    map_at_3 = 0.0\n",
    "    for u in range(U):\n",
    "        user_preds = predictions[u]\n",
    "        user_true = true_items[u]\n",
    "        user_results = [1 if item == user_true else 0 for item in user_preds]\n",
    "        for k in range(min(len(user_preds), 3)):\n",
    "            map_at_3 += precision_at_k(user_results, k+1) * user_results[k]\n",
    "    return map_at_3 / U\n",
    "\n",
    "# NB the calculation here is missing the short circuiting of the precision_at_k function\n",
    "# This would be needed if the model output multiple copies of the same answer\n",
    "# This is specced in the MAP@3 calculation description in the competition rules\n",
    "# https://www.kaggle.com/competitions/kaggle-llm-science-exam\n",
    "# redo it here for clarity\n",
    "# The code that invokes this function won't let this happen as it uses [cols][np.argsort(preplexity)]\n",
    "# which won't return the same index for tied values. Still though, this feels like a bug in the calc so\n",
    "# worth noting\n",
    "# TODO test this\n",
    "def MAP_at_3(predictions, true_items):\n",
    "    \"\"\"Score is mean average precision at 3\"\"\"\n",
    "    U = len(predictions)\n",
    "    map_at_3 = 0.0\n",
    "    for u in range(U):\n",
    "        user_preds = predictions[u]\n",
    "        user_true = true_items[u]\n",
    "        user_results = [1 if item == user_true else 0 for item in user_preds]\n",
    "        for k in range(min(len(user_preds), 3)):\n",
    "            map_at_3 += precision_at_k(user_results, k+1) * user_results[k]\n",
    "            if user_results[k] == 1:\n",
    "                break\n",
    "    return map_at_3 / U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = Perplexity()\n",
    "candidate = np.array(['A', 'B', 'C', 'D', 'E'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/takamichitoda/llm-perplexity-ranking-ensemble\n",
    "def infer(model_name, infer_df):\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    perps, scores = [], []\n",
    "    for _, row in tqdm(infer_df.iterrows(), total=len(infer_df)):\n",
    "        inp = row[\"prompt\"]\n",
    "        cands = [f\"Question: {inp}\\n Answer: {row[c]}\" for c in candidate]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(cands, return_tensors=\"pt\", padding=True, truncation=True).to(f\"cuda:{model.device.index}\")\n",
    "            output = model(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask,)\n",
    "            output = output.logits\n",
    "            labels = inputs.input_ids\n",
    "            labels.masked_fill_(~inputs[\"attention_mask\"].bool(), -100)\n",
    "            perp = [perplexity(output[i].unsqueeze(0), labels[i].unsqueeze(0)).cpu() for i in range(len(cands))]\n",
    "\n",
    "        # save perplexity not the sorted predictions for each question so we can use it for ensembling\n",
    "        perps.append(perp)\n",
    "\n",
    "        # calculate the MAP@3 score for each question, MAP_at_3 expects a list of lists and a list of answers\n",
    "        # but we're doing them individually here so we need to wrap them in lists\n",
    "        preds = candidate[np.argsort(np.asarray(perp))]\n",
    "        score = MAP_at_3([preds], [row[\"answer\"]])\n",
    "        scores.append(score)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return perps, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0986)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO delete\n",
    "perplexity(torch.tensor([[[0.1, 0.1, 0.1], [0.1, 0.1, 0.1]]]), torch.tensor([[0, 0]])).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569e06c25bfc4f228764844a48704d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/dom/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|██████████| 200/200 [02:42<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/Mistral-7B-v0.1 MAP@3: 0.7208333333333333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b848b3172bf74094b833f2b2061fed61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:33<00:00,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-ai/Yi-6B MAP@3: 0.6283333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_perps = []\n",
    "for model_name in MODELS:\n",
    "    perps, scores = infer(model_name, test_df)\n",
    "    model_perps.append(perps)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"{model_name} MAP@3: {np.mean(scores)}\")\n",
    "\n",
    "perps = np.array(model_perps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.9469231, 1.8957705, 2.1129675, 1.846193 , 1.9246615],\n",
       "       [2.5738037, 2.6259425, 2.5766556, 2.5453894, 2.6985378],\n",
       "       [2.350564 , 2.1702247, 2.1937566, 2.3108087, 2.1098208],\n",
       "       [3.1203482, 2.9559467, 2.8621874, 3.0236456, 3.1727817],\n",
       "       [1.7092881, 1.6327369, 1.7643096, 1.5720744, 1.7078724],\n",
       "       [2.2709692, 2.0655148, 1.9405072, 1.9165689, 2.0688221],\n",
       "       [1.9622561, 2.2139924, 1.9954762, 2.094616 , 2.4284868],\n",
       "       [1.6194817, 1.6595483, 1.6707814, 1.7516892, 1.5812576],\n",
       "       [3.0847971, 2.9435732, 3.0494156, 3.3751962, 3.3799398],\n",
       "       [2.4534338, 2.6544611, 2.344752 , 2.5890644, 2.217326 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perps[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "perps_avg = perps[0] if len(perps) == 1 else np.mean(perps, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see whether the ensembling has actually helped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@3: 0.7208333333333333\n",
      "MAP@3: 0.6283333333333333\n",
      "MAP@3: 0.675\n"
     ]
    }
   ],
   "source": [
    "for _perps in [perps[0], perps[1], perps_avg]:\n",
    "    _scores = []\n",
    "    for i, row in test_df.iterrows():\n",
    "        # _perp = perps_avg[i]\n",
    "        perp = _perps[i]\n",
    "        preds = candidate[np.argsort(np.asarray(perp))]\n",
    "        m = MAP_at_3([preds], [row.answer])\n",
    "        _scores.append(m)\n",
    "\n",
    "    print(f\"MAP@3: {np.mean(_scores)}\")\n",
    "    # [\" \".join(candidate[np.argsort(perp)][:3]) for perp in _perps]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No! The `\"01-ai/Yi-6B\"` model isn't as strong as `Mistral-7b` for this task. There are likely to be combinations that help here, but working that out can be for another time when it's actually needed. The code is here and working.\n",
    "\n",
    "For now, just go with the Mistral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv(data_dir / 'sample_submission.csv')\n",
    "\n",
    "# Uncomment to use ensembled predictions\n",
    "# sub_df[\"prediction\"] = [\" \".join(candidate[np.argsort(perp)][:3]) for perp in perps_avg]\n",
    "\n",
    "# otherwise use the first model\n",
    "sub_df[\"prediction\"] = [\" \".join(candidate[np.argsort(perp)][:3]) for perp in perps[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>D B E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D A C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>E B C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>C B D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>D B E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id prediction\n",
       "0   0      D B E\n",
       "1   1      D A C\n",
       "2   2      E B C\n",
       "3   3      C B D\n",
       "4   4      D B E"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv(data_dir / 'submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>D B E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D A C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>E B C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>C B D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>D B E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>E C B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>C B D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>197</td>\n",
       "      <td>B C A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>198</td>\n",
       "      <td>D C B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>199</td>\n",
       "      <td>E A B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id prediction\n",
       "0      0      D B E\n",
       "1      1      D A C\n",
       "2      2      E B C\n",
       "3      3      C B D\n",
       "4      4      D B E\n",
       "..   ...        ...\n",
       "195  195      E C B\n",
       "196  196      C B D\n",
       "197  197      B C A\n",
       "198  198      D C B\n",
       "199  199      E A B\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(data_dir / \"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING -- TODO delete below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "TODO: delete below here",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/workspace/perplexity_ranking_ensemble.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6169646576227d@ssh-remote%2Bgcp/workspace/perplexity_ranking_ensemble.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTODO: delete below here\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: TODO: delete below here"
     ]
    }
   ],
   "source": [
    "assert 0, \"TODO: delete below here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_scores = []\n",
    "# _perps = perps_avg\n",
    "_perps = perps[0]\n",
    "# _perps = perps[1]\n",
    "for i, row in enumerate(test_df.iterrows()):\n",
    "    _row = test_df.iloc[i]\n",
    "    # _perp = perps_avg[i]\n",
    "    _perp = _perps[i]\n",
    "    _preds = candidate[np.argsort(np.asarray(_perp))]\n",
    "    m = MAP_at_3([_preds], [_row[\"answer\"]])\n",
    "    _scores.append(m)\n",
    "\n",
    "print(f\"MAP@3: {np.mean(_scores)}\")\n",
    "[\" \".join(candidate[np.argsort(perp)][:3]) for perp in _perps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "968"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "del inputs\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model mistralai/Mistral-7B-v0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76552ad18d8d4ec392705420f8fc27f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = MODELS[0]\n",
    "print(f\"Using model {model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s></s></s></s></s></s></s></s></s><s> Question: Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?\n",
      " Answer: MOND is a theory that reduces the observed missing baryonic mass in galaxy clusters by postulating the existence of a new form of matter called \"fuzzy dark matter.\" tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "row = test_df.iloc[0]\n",
    "inp = row[\"prompt\"]\n",
    "cands = [f\"Question: {inp}\\n Answer: {row[c]}\" for c in candidate]\n",
    "inputs = tokenizer(cands, return_tensors=\"pt\", padding=True, truncation=True).to(f\"cuda:{model.device.index}\")\n",
    "print(tokenizer.decode(inputs.input_ids[0]), inputs.attention_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/home/dom/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|██████████| 200/200 [02:02<00:00,  1.63it/s]\n"
     ]
    }
   ],
   "source": [
    "perps_testing = []\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    inp = row[\"prompt\"]\n",
    "    cands = [f\"Question: {inp}\\n Answer: {row[c]}\" for c in candidate]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(cands, return_tensors=\"pt\", padding=True, truncation=True).to(f\"cuda:{model.device.index}\")\n",
    "        output = model(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask,)\n",
    "        output = output.logits\n",
    "        labels = inputs.input_ids\n",
    "        labels.masked_fill_(~inputs[\"attention_mask\"].bool(), -100)\n",
    "        perp = [perplexity(output[i].unsqueeze(0), labels[i].unsqueeze(0)).cpu() for i in range(len(cands))]\n",
    "        perps_testing.append(perp)\n",
    "        # print(perp)\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['E', 'A', 'B', 'C', 'D'], dtype='<U1'), 'C')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate[np.argsort(perp)], row.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10016"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perps = []\n",
    "for model_name in [MODELS[0]]:\n",
    "    perps, scores = infer(model_name, test_df[:10])\n",
    "    model_perps.append(perps)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"{model_name} MAP@3: {np.mean(scores)}\")\n",
    "\n",
    "perps = np.array(model_perps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
